import asyncio
import edge_tts
from config import Config
from pydub import AudioSegment
import os
import torch
import soundfile as sf

class TTSProcessor:
    """Base class for TTS, defaults to EdgeTTS for speed/efficiency."""
    def __init__(self, voice="en-US-ChristopherNeural"):
        self.voice = voice

    async def _generate_audio(self, text, output_file):
        communicate = edge_tts.Communicate(text, self.voice)
        await communicate.save(output_file)

    async def generate_full_audio(self, segments, output_path):
        print(f"ğŸ—£ï¸ Generating TTS audio for {len(segments)} segments via Edge-TTS...")
        
        # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘ï¼Œé¿å…è§¦å‘ API é™åˆ¶æˆ–è¿‡è½½
        semaphore = asyncio.Semaphore(10)
        temp_dir = Config.TEMP_DIR / "tts_segments"
        temp_dir.mkdir(exist_ok=True)

        async def _process_segment(i, text):
            async with semaphore:
                temp_file = temp_dir / f"seg_{i:04d}.mp3"
                await self._generate_audio(text, str(temp_file))
                return i, temp_file

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ TTS è¯·æ±‚
        tasks = [
            _process_segment(i, seg['text']) 
            for i, seg in enumerate(segments)
        ]
        
        print(f"â³ Downloading segments in parallel...")
        results = await asyncio.gather(*tasks)
        # æŒ‰ç´¢å¼•æ’åºç¡®ä¿é¡ºåºæ­£ç¡®
        results.sort(key=lambda x: x[0])

        print(f"ğŸ§© Combining audio segments with Precise Time Matching...")
        combined_audio = AudioSegment.empty()
        
        for (i, temp_file), seg in zip(results, segments):
            start_ms = int(seg['start'] * 1000)
            end_ms = int(seg['end'] * 1000)
            target_duration = end_ms - start_ms
            
            # 1. å¡«å……é™éŸ³ç›´åˆ°å½“å‰ç‰‡æ®µå¼€å§‹
            if len(combined_audio) < start_ms:
                silence_gap = start_ms - len(combined_audio)
                combined_audio += AudioSegment.silent(duration=silence_gap)
            
            # 2. è¯»å–ç”Ÿæˆçš„éŸ³é¢‘
            seg_audio = AudioSegment.from_mp3(temp_file)
            actual_duration = len(seg_audio)
            
            # 3. åŠ¨æ€å€é€Ÿå¤„ç† (Time Stretching)
            # å¦‚æœç¿»è¯‘åçš„æ–‡æœ¬å¤ªé•¿ï¼Œå¯¼è‡´éŸ³é¢‘è¶…è¿‡äº†åŸè§†é¢‘ç‰‡æ®µçš„æ—¶é•¿ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œå˜é€Ÿ
            # å‚è€ƒå¼€æºé¡¹ç›®æœ€ä½³å®è·µï¼šå€é€ŸèŒƒå›´å»ºè®®åœ¨ 0.8x åˆ° 1.5x ä¹‹é—´ï¼Œå¦åˆ™å£°éŸ³ä¼šå¤±çœŸä¸¥é‡
            if actual_duration > target_duration and target_duration > 0:
                speed_factor = actual_duration / target_duration
                # é™åˆ¶æœ€å¤§å€é€Ÿï¼Œé¿å…å˜æˆâ€œèŠ±æ —é¼ â€å£°éŸ³
                if speed_factor > 1.5:
                    print(f"âš ï¸ Warning: Segment {i} is too long ({actual_duration}ms vs {target_duration}ms). Capping speed factor at 1.5x.")
                    speed_factor = 1.5
                
                # ä½¿ç”¨ pydub çš„å˜é€ŸåŠŸèƒ½ï¼ˆæ³¨æ„ï¼šè¿™ç§å˜é€Ÿä¼šæ”¹å˜éŸ³è°ƒï¼Œåç»­å¯ä»¥è€ƒè™‘ç”¨ ffmpeg atempo ä¼˜åŒ–æ— æŸéŸ³è°ƒå˜é€Ÿï¼‰
                seg_audio = seg_audio.speedup(playback_speed=speed_factor, chunk_size=150, crossfade=25)
            
            # 4. è£å‰ªå¤šä½™éƒ¨åˆ†æˆ–ä¿ç•™ï¼ˆè§†é€»è¾‘è€Œå®šï¼Œè¿™é‡Œæˆ‘ä»¬æ ¹æ® start_ms å¼ºåˆ¶å¯¹é½ï¼‰
            # ç¡®ä¿ä¸è¦†ç›–ä¸‹ä¸€ä¸ªç‰‡æ®µï¼ˆé™¤éä¸å¾—ä¸è¦†ç›–ï¼‰
            combined_audio = combined_audio[:start_ms] + seg_audio
            
            # ç«‹å³åˆ é™¤å°çš„ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file): os.remove(temp_file)
            
        combined_audio.export(output_path, format="wav")
        print(f"âœ… Audio generated with sync protection: {output_path}")
        return output_path

class IndexTTSProcessor:
    """Advanced TTS using Index-TTS2 with CUDA acceleration and voice cloning."""
    def __init__(self, device="cuda"):
        self.device = device
        self.model_name = "IndexTeam/IndexTTS-2"
        self.model = None
        # In a real implementation, we would load the model from HF here
        # For the prototype, we will assume the environment is setup via setup_colab.sh
    
    def load_model(self):
        if self.model is None:
            print("â³ Loading Index-TTS2 Model on GPU...")
            # Placeholder for actual model loading logic
            # This would typically involve:
            # self.model = AutoModel.from_pretrained(self.model_name).to(self.device).half()
            print("âœ… Index-TTS2 Loaded (CUDA/FP16).")

    def generate_with_cloning(self, text, ref_audio_path, output_path):
        """Uses a reference audio to clone voice and generate speech."""
        self.load_model()
        print(f"ğŸ™ï¸ Cloning voice from {ref_audio_path}...")
        # Implementation of Index-TTS2's inference logic goes here
        # It uses CUDA to match the duration and prosody.
        pass

    def unload(self):
        if self.model:
            del self.model
            torch.cuda.empty_cache()
            print("ğŸ—‘ï¸ Index-TTS2 Unloaded from VRAM.")
