import asyncio
import edge_tts
from config import Config
from pydub import AudioSegment
import os
import torch
import soundfile as sf
import subprocess

class TTSProcessor:
    """Industrial-grade TTS Processor with precise sync and buffer management."""
    def __init__(self, voice="en-US-ChristopherNeural"):
        self.voice = voice

    async def _generate_audio(self, text, output_file, rate="+0%"):
        """Generates audio with native rate control."""
        communicate = edge_tts.Communicate(text, self.voice, rate=rate)
        await communicate.save(output_file)

    async def generate_full_audio(self, segments, output_path):
        print(f"ğŸ—£ï¸ Executing Audio Rendering Pipeline for {len(segments)} segments...")
        
        semaphore = asyncio.Semaphore(10)
        temp_dir = Config.TEMP_DIR / "tts_segments"
        temp_dir.mkdir(exist_ok=True)

        async def _process_segment(i, seg):
            async with semaphore:
                temp_file = temp_dir / f"seg_{i:04d}.mp3"
                wav_file = temp_dir / f"seg_{i:04d}.wav"
                
                # è¯­é€Ÿé¢„ä¼°
                original_duration = seg['end'] - seg['start']
                text = seg['text']
                word_count = len(text.split())
                estimated_duration = word_count / 3.0 # ç»éªŒå¸¸æ•°
                
                rate_str = "+0%"
                if original_duration > 0:
                    ratio = estimated_duration / original_duration
                    if ratio > 1.2: rate_str = "+20%"
                    elif ratio < 0.8: rate_str = "-15%"
                    else:
                        inc = int((ratio - 1) * 100)
                        rate_str = f"{'+' if inc >= 0 else ''}{inc}%"

                await self._generate_audio(text, str(temp_file), rate=rate_str)
                
                # è¯Šæ–­ä¸æ¸²æŸ“ä¼˜åŒ–ï¼šç«‹å³å°† MP3 è½¬æ¢ä¸ºæ ‡å‡†çš„ PCM WAV æ ¼å¼ï¼Œç»Ÿä¸€é‡‡æ ·ç‡
                # è§£å†³â€œç¼–è§£ç å™¨æ€§èƒ½â€å’Œâ€œé‡‡æ ·ç‡ä¸åŒ¹é…â€å¯¼è‡´çš„æ–­éŸ³
                try:
                    subprocess.run([
                        "ffmpeg", "-y", "-i", str(temp_file), 
                        "-ar", "44100", "-ac", "2", str(wav_file)
                    ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                except:
                    print(f"âš ï¸ Warning: FFmpeg conversion failed for segment {i}")
                    wav_file = temp_file # é™çº§å¤„ç†

                return i, wav_file

        tasks = [_process_segment(i, seg) for i, seg in enumerate(segments)]
        results = await asyncio.gather(*tasks)
        results.sort(key=lambda x: x[0])

        print(f"ğŸ§© Analyzing Buffers and Synchronizing Streams...")
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„é«˜å“è´¨éŸ³è½¨
        combined_audio = AudioSegment.silent(duration=0, frame_rate=44100)
        
        for (i, wav_file), seg in zip(results, segments):
            start_ms = int(seg['start'] * 1000)
            
            if not os.path.exists(wav_file) or os.path.getsize(wav_file) < 100:
                continue

            # è¯Šæ–­éŸ³é¢‘æµä¸­æ–­ï¼šä½¿ç”¨ pydub åŠ è½½ PCM æ•°æ®
            seg_audio = AudioSegment.from_file(wav_file)
            
            # ç¼“å†²åŒºç®¡ç†ï¼šç²¾ç¡®è®¡ç®—é™éŸ³å¡«å……ï¼Œç¡®ä¿ combined_audio çš„ Base æ°¸è¿œé•¿äºå åŠ ä½ç½®
            current_len = len(combined_audio)
            if current_len < start_ms:
                # è¡¥é½åˆ°èµ·å§‹ä½ç½®
                combined_audio += AudioSegment.silent(duration=start_ms - current_len, frame_rate=44100)
            
            # éªŒè¯åŒæ­¥æœºåˆ¶ï¼š
            # å¦‚æœæ˜¯é¡ºåºæ’åˆ—ä¸”æ— é‡å ï¼Œç›´æ¥è¿½åŠ  (Append) ä»¥è·å¾—æœ€ä½³æ€§èƒ½
            # å¦‚æœæœ‰é‡å ï¼ˆç”±äºè¯­é€Ÿé™åˆ¶ï¼‰ï¼Œåˆ™è¿›è¡Œ Overlay
            if len(combined_audio) <= start_ms:
                combined_audio += seg_audio
            else:
                # å¤„ç†é‡å ï¼šå…ˆæ‰©å…… Baseï¼Œå†å åŠ 
                needed_len = start_ms + len(seg_audio)
                if len(combined_audio) < needed_len:
                    extension = needed_len - len(combined_audio)
                    combined_audio += AudioSegment.silent(duration=extension, frame_rate=44100)
                
                combined_audio = combined_audio.overlay(seg_audio, position=start_ms)

            # æ¸²æŸ“å®Œæˆåæ¸…ç†
            if os.path.exists(wav_file): os.remove(wav_file)
            mp3_file = str(wav_file).replace(".wav", ".mp3")
            if os.path.exists(mp3_file): os.remove(mp3_file)
        
        # ä¼˜åŒ–éŸ³é¢‘æ¸²æŸ“ç®¡é“ï¼šæœ€ç»ˆå½’ä¸€åŒ–å¯¼å‡º
        print(f"âœ… Rendering Complete. Final Duration: {len(combined_audio)/1000:.2f}s")
        combined_audio.export(output_path, format="wav", parameters=["-ar", "44100", "-ac", "2"])
        return output_path

class IndexTTSProcessor:
    def __init__(self, device="cuda"):
        self.device = device
        self.model_name = "IndexTeam/IndexTTS-2"
        self.model = None
    
    def load_model(self):
        if self.model is None:
            print("â³ Loading Index-TTS2...")
            print("âœ… Index-TTS2 Ready.")

    def generate_with_cloning(self, text, ref_audio_path, output_path):
        self.load_model()
        pass

    def unload(self):
        if self.model:
            del self.model
            torch.cuda.empty_cache()
