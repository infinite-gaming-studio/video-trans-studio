import gc
import torch
from faster_whisper import WhisperModel
from config import Config

class ASRProcessor:
    def __init__(self):
        self.model = None

    def load_model(self):
        if self.model is None:
            print(f"â³ Loading Whisper Model ({Config.WHISPER_MODEL_SIZE})...")
            self.model = WhisperModel(
                Config.WHISPER_MODEL_SIZE, 
                device=Config.DEVICE, 
                compute_type=Config.WHISPER_COMPUTE_TYPE
            )
            print("âœ… Whisper Model Loaded.")

    def transcribe(self, audio_path):
        self.load_model()
        print(f"ğŸ™ï¸ Transcribing: {audio_path}...")
        
        # ä¼˜åŒ–å‚æ•°ï¼šå¢åŠ  word_timestamps å’Œæ›´ç²¾ç»†çš„ vad æ§åˆ¶
        segments, info = self.model.transcribe(
            audio_path, 
            beam_size=5, 
            vad_filter=True,
            vad_parameters=dict(min_silence_duration_ms=500),
            word_timestamps=True,  # å¼€å¯è¯çº§æ—¶é—´æˆ³ï¼Œæ–¹ä¾¿åç»­ç²¾ç»†åŒ–å¤„ç†
            initial_prompt="ä»¥ä¸‹æ˜¯æ™®é€šè¯ï¼Œè¯·åŠ æ ‡ç‚¹ç¬¦å·ã€‚", # å¼ºåˆ¶è¦æ±‚å¸¦æ ‡ç‚¹ï¼Œæœ‰åŠ©äºæ–­å¥
        )
        
        result_segments = []
        for segment in segments:
            # å¦‚æœå•å¥å¤ªé•¿ï¼ˆæ¯”å¦‚è¶…è¿‡ 10 ç§’ï¼‰ï¼Œåœ¨è¿™é‡Œå¯ä»¥åšè¿›ä¸€æ­¥çš„é€»è¾‘åˆ†å‰²
            # ç›®å‰å…ˆè¿›è¡ŒåŸºç¡€æ¸…ç†
            text = segment.text.strip()
            if not text:
                continue
                
            result_segments.append({
                "start": segment.start,
                "end": segment.end,
                "text": text
            })
            
        print(f"âœ… Transcription complete. Detected language: {info.language}")
        return result_segments

    def unload(self):
        """Free up VRAM for the next step."""
        if self.model:
            del self.model
            self.model = None
            gc.collect()
            torch.cuda.empty_cache()
            print("ğŸ—‘ï¸ Whisper Model Unloaded.")
